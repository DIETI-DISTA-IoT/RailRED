{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3236f3be-b967-4666-bcf0-1a45393cd4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2accc9eb-f13b-4b87-9a6d-b5fc36b7f2d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.distance import geodesic\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de254d-2ce0-4fc8-84dd-578d55b9c9e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1449fb6a-00a9-4900-9782-124af670f9fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 402 297\n",
      "580 402 295\n"
     ]
    }
   ],
   "source": [
    "main_df = pd.read_csv('dataset.csv')\n",
    "\n",
    "print(\n",
    "    main_df['Codice'].nunique(), main_df['Descrizione'].nunique(), main_df['Nome'].nunique()\n",
    "    )\n",
    "\n",
    "main_df = main_df.dropna(subset=['Descrizione'])  # clean\n",
    "main_df['Timestamp'] = pd.to_datetime(main_df['Timestamp'])\n",
    "main_df['Timestamp segnale'] = pd.to_datetime(main_df['Timestamp segnale'])\n",
    "\n",
    "main_df = main_df.sort_values('Timestamp segnale')\n",
    "\n",
    "# avoid blocks of signals... (take only the first from a continguous group of signals)\n",
    "main_df = main_df.loc[main_df['Descrizione'] != \\\n",
    "                              main_df['Descrizione'].shift(1)].copy()\n",
    "\n",
    "\n",
    "main_df = main_df.drop_duplicates(subset=[\n",
    "    'Descrizione', 'Timestamp segnale'], keep='first')\n",
    "\n",
    "unique_value_columns = main_df.nunique() == 1  # Identify columns with a single unique value\n",
    "main_df = main_df.loc[:, ~unique_value_columns]  # Drop these columns\n",
    "\n",
    "# we are going to use all the features, potentially.\n",
    "# So there might be an indicator of \"missing\" value which is feedable to a neural net\n",
    "main_df.fillna(-1, inplace=True)\n",
    "\n",
    "print(\n",
    "    main_df['Codice'].nunique(), main_df['Descrizione'].nunique(), main_df['Nome'].nunique()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932cce72-edd7-4fa0-9ed1-b37b715d1ad6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Going beyond time...\n",
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a65a1-49b3-4cec-9e12-96745e4cf635",
   "metadata": {},
   "source": [
    "\n",
    "Let's look at the Bayes Rule:\n",
    "\n",
    "$$P(anomaly | event)  =   \\frac{ P( event | anomaly) \\cdot P( anomaly) }{ P(event)} $$\n",
    "\n",
    "\n",
    "For stability, we will use:\n",
    "\n",
    "$$ log[P(anomaly | event)]  =   log[\\frac{ P( event | anomaly) \\cdot P( anomaly) }{ P(event)}] $$\n",
    "\n",
    "So the posteriors can be computed as:\n",
    "\n",
    "$$ log[P(anomaly | event)]  =   log[ P( event | anomaly)] + log[ P( anomaly) ] - log[ P(event) ] $$\n",
    "\n",
    "or, conversely, the likelihoods can be computed as:\n",
    "\n",
    "$$ log[P(event | anomaly)]  =   log[ P( anomaly | event)] - log[ P( anomaly) ] + log[ P(event) ] $$\n",
    "\n",
    "\n",
    "\n",
    "In general, the predictive maintenance task can be see as the task of approximating posteriors (or likelihoods). \n",
    "\n",
    "- What really matters is what we exactly mean by the _conditional probabilities_, be they posteriors or likelihoods. \n",
    "\n",
    "- Notice that, for many of the definition we choose, any MLP with non linearities can in principle \"memorise\" the whole dataset and solve this problem given enough training, so we need to be careful and follow the Ockam's razor principle: we will add \"information\" to the \"conditioning event\" little by little, and try to see if we have better predictors w.r.t the only-time setting above.\n",
    "\n",
    "Let's reason on extreme settings: \n",
    "-> simplest conditioning event: a flag indicating the event happened (like in the previous setting)\n",
    "-> simplest posterior to predict: the next time of occurrence of an anomaly (like in the previous setting)\n",
    "-> most complex conditioning event encoding: the whole dataset before the current time.\n",
    "-> most complex posterior to predict: the whole dataset after the current time.\n",
    "\n",
    "We will fix the simplest posterior: the next time of occurrence of an anomaly (like in the previous setting) and try to iteratively complexify the conditioning event encoding...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108652fd-ddac-4156-8e64-fe63c58d42b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### First thing: compute the priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80db5587-e7d0-4a84-a3ee-83de8544e4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e98243a796d438e88d29e6d3a79367d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample space dim 28079\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To compute prior probabilites on events/anomalies, we use the frequentist definition...\n",
    "\"\"\"\n",
    "# we'll only consider events with a minimum occurrence count:\n",
    "analysis_treshold = 300\n",
    "\n",
    "# Grouping by 'Category'\n",
    "grouped_prior = main_df.groupby('Descrizione')\n",
    "\n",
    "# Creating a dictionary of DataFrames\n",
    "priors = {category: group\n",
    "          for category, group in grouped_prior\n",
    "          if len(group) > analysis_treshold}\n",
    "\n",
    "sample_space_dim = 0\n",
    "\n",
    "for prior in tqdm(priors):\n",
    "    sample_space_dim += len(priors[prior])\n",
    "\n",
    "print('sample space dim', sample_space_dim)\n",
    "\n",
    "# Creating a dictionary of DataFrames\n",
    "prior_probs = {\n",
    "    prior[0]: len(prior[1])/sample_space_dim\n",
    "    for prior in priors.items()}\n",
    "\n",
    "# verify we have done things right:\n",
    "summation = 0\n",
    "for item in prior_probs.values():\n",
    "    summation += item\n",
    "assert 0.9999 <= summation <= 1.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbdcd4e-b84f-4385-8f8d-a0d3b9e3fca9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Second: get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db12190-bcdc-47aa-a3e7-fa48b3dccc75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of anomalies to analyse:  15 num of events:  25\n"
     ]
    }
   ],
   "source": [
    "# our inferences are going to be made from this cleaned dataset:\n",
    "main_df = pd.concat([df for df in priors.values()])\n",
    "main_df.sort_values('Timestamp', inplace=True)\n",
    "\n",
    "# we differentiate between events and anomalies\n",
    "anomalies = {}\n",
    "total_labels = list(priors.keys())\n",
    "\n",
    "for label in total_labels:\n",
    "    if 'guasto' in label or \\\n",
    "            'abbassamento' in label or \\\n",
    "            'ancanza' in label or \\\n",
    "            'non presente' in label or \\\n",
    "            'fuori servizio' in label or \\\n",
    "            'Mancata ' in label or \\\n",
    "            'mergenza ' in label or \\\n",
    "            'Guasto ' in label or \\\n",
    "            'insufficiente ' in label or \\\n",
    "            'Perdita' in label:\n",
    "        anomalies[label] = priors[label]\n",
    "        del priors[label]\n",
    "\n",
    "generic_events = priors\n",
    "\n",
    "print('number of anomalies to analyse: ', len(anomalies),\n",
    "      'num of events: ', len(generic_events))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee494af-524e-42db-aed5-c81c69acb8cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### third: categorical encoding..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f3489ea-cf73-47c9-99d9-28c2255b8a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " we have 93 different codes\n",
      " we have 57 different names\n",
      " we have 40 different descs\n",
      " we have 8 different systems\n",
      " we have 35 different components\n"
     ]
    }
   ],
   "source": [
    "codes = main_df['Codice'].unique()\n",
    "print(f' we have {len(codes)} different codes')\n",
    "code_encoder = {code_tuple[1]: code_tuple[0] for code_tuple in enumerate(codes)}\n",
    "code_decoder = {code_tuple[0]: code_tuple[1] for code_tuple in enumerate(codes)}\n",
    "\n",
    "names = main_df['Nome'].unique()\n",
    "print(f' we have {len(names)} different names')\n",
    "name_encoder = {name_tuple[1]: name_tuple[0] for name_tuple in enumerate(names)}\n",
    "name_decoder = {name_tuple[0]: name_tuple[1] for name_tuple in enumerate(names)}\n",
    "\n",
    "descs = main_df['Descrizione'].unique()\n",
    "print(f' we have {len(descs)} different descs')\n",
    "desc_encoder = {desc_tuple[1]: desc_tuple[0] for desc_tuple in enumerate(descs)}\n",
    "desc_decoder = {desc_tuple[0]: desc_tuple[1] for desc_tuple in enumerate(descs)}\n",
    "\n",
    "sists = main_df['Sistema'].unique()\n",
    "print(f' we have {len(sists)} different systems')\n",
    "sist_encoder = {sist_tuple[1]: sist_tuple[0] for sist_tuple in enumerate(sists)}\n",
    "sist_decoder = {sist_tuple[0]: sist_tuple[1] for sist_tuple in enumerate(sists)}\n",
    "\n",
    "comps = main_df['Componente'].unique()\n",
    "print(f' we have {len(comps)} different components')\n",
    "comp_encoder = {comp_tuple[1]: comp_tuple[0] for comp_tuple in enumerate(comps)}\n",
    "comp_decoder = {comp_tuple[0]: comp_tuple[1] for comp_tuple in enumerate(comps)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f5376-7676-419d-87d8-790ace0a8775",
   "metadata": {
    "tags": []
   },
   "source": [
    "### let's play around with a specific anomaly prediction case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08eda34a-c5a4-49de-9b66-9e9242edf91c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# computing relative times...\n",
    "def preprocess_chunk(chunk):\n",
    "\n",
    "    chunk['rel_time'] = chunk.iloc[-1]['Timestamp'] - chunk['Timestamp']\n",
    "    chunk['rel_time'] = pd.to_timedelta(chunk['rel_time']).apply(lambda x: \\\n",
    "                                                  x.total_seconds() * 10**6 \\\n",
    "                                                  + x.microseconds)\n",
    "    chunk['rel_time'] = chunk['rel_time'] / 10**6\n",
    "    chunk.drop(\n",
    "        columns=['Timestamp', 'Timestamp chiusura', 'Timestamp segnale', 'Posizione'],\n",
    "        inplace=True)\n",
    "\n",
    "    chunk['Codice'] = chunk['Codice'].apply(\n",
    "        lambda code_str: code_encoder[code_str])\n",
    "    chunk['Nome'] = chunk['Nome'].apply(\n",
    "        lambda name_str: name_encoder[name_str])\n",
    "    chunk['Descrizione'] = chunk['Descrizione'].apply(\n",
    "        lambda desc_str: desc_encoder[desc_str])\n",
    "    chunk['Sistema'] = chunk['Sistema'].apply(\n",
    "        lambda sist_str: sist_encoder[sist_str])\n",
    "    chunk['Componente'] = chunk['Componente'].apply(\n",
    "        lambda comp_str: comp_encoder[comp_str])\n",
    "\n",
    "    return chunk\n",
    "\n",
    "\n",
    "def get_chunks(current_anomaly):\n",
    "\n",
    "    # List to hold the chunks\n",
    "    chunks = []\n",
    "\n",
    "    # Initialize the starting index\n",
    "    start_idx = 0\n",
    "\n",
    "    min_len = 5\n",
    "    # Iterate through the DataFrame to identify chunk boundaries\n",
    "    for idx, value in tqdm(enumerate(main_df['Descrizione']), total=len(main_df)):\n",
    "        if value == current_anomaly:\n",
    "            if idx - start_idx >= min_len:\n",
    "                chunk = main_df.iloc[start_idx:idx+1].copy()\n",
    "                chunk = preprocess_chunk(chunk)\n",
    "                chunks.append(chunk)\n",
    "            start_idx = idx + 1\n",
    "\n",
    "    print(f'Generated {len(chunks)} data samples for anomaly: {current_anomaly}')\n",
    "    print('length of samples: ')\n",
    "    pd.Series([len(chunk) for chunk in chunks]).describe()\n",
    "\n",
    "    print('Tranining a scaler for interarrival times...')\n",
    "    labels = []\n",
    "    for chunk in tqdm(chunks):\n",
    "        labels.extend(list(chunk['rel_time'].unique()))\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    intertimes_rs = scaler.fit_transform(np.array(labels).reshape(-1, 1))\n",
    "    return chunks, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291afe3d-6a6c-49df-8286-cbf835855a16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Core architecture: attention is all you need... (to deal with multiple time-series records...) eventually, with some recurrency...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "796d2e66-9e2b-45b6-b882-a218e063fccb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, embed_dim)\n",
    "        # Transpose x to match the MultiheadAttention input shape: (seq_len, batch_size, embed_dim)\n",
    "        x = x.transpose(0, 1)\n",
    "        attn_output, _ = self.multihead_attn(x, x, x)  # Self-attention\n",
    "        # Transpose back to (batch_size, seq_len, embed_dim)\n",
    "        attn_output = attn_output.transpose(0, 1)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PredictorBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 mlp_hidden_dim,\n",
    "                 recurrent=False,\n",
    "                 dropout=0.1):\n",
    "\n",
    "        super(PredictorBlock, self).__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(input_dim)  # LayerNorm over the input features\n",
    "        self.encoding_layer = nn.Linear(input_dim, embed_dim)\n",
    "        self.self_attention = SelfAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.recurrent = recurrent\n",
    "        if self.recurrent:\n",
    "            self.gru = nn.GRU(input_size=embed_dim,\n",
    "                              hidden_size=embed_dim,\n",
    "                              num_layers=1,\n",
    "                              batch_first=True)\n",
    "        self.mlp = MLP(embed_dim, mlp_hidden_dim, 1, dropout)  # Output dimension is 1 for scalar output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        x = self.layer_norm(x)  # Apply LayerNorm\n",
    "        x = self.encoding_layer(x)\n",
    "        x = self.self_attention(x)  # (batch_size, seq_len, embed_dim)\n",
    "        if self.recurrent:\n",
    "            x, _ = self.gru(x)\n",
    "            x = x[:, -1, :]\n",
    "        else:\n",
    "            x = x[:, -1]\n",
    "        output = self.mlp(x)  # (batch_size, 1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c3797c1-ebd0-4916-827e-8e232c2c99cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AnomalyPredictor(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim,\n",
    "            embed_dim=16,\n",
    "            num_heads=1,\n",
    "            mlp_hidden_dim=8,\n",
    "            device='cuda:0',\n",
    "            include_numeric_feats=False,\n",
    "            recurrent=False,\n",
    "            dropout=0.3):\n",
    "\n",
    "        super(AnomalyPredictor, self).__init__()\n",
    "\n",
    "        self.code_embedding_layer = nn.Embedding(\n",
    "            num_embeddings=len(codes),\n",
    "            embedding_dim=7)\n",
    "        self.name_embedding_layer = nn.Embedding(\n",
    "            num_embeddings=len(names),\n",
    "            embedding_dim=6)\n",
    "        self.desc_embedding_layer = nn.Embedding(\n",
    "            num_embeddings=len(descs),\n",
    "            embedding_dim=6)\n",
    "        self.sist_embedding_layer = nn.Embedding(\n",
    "            num_embeddings=len(sists),\n",
    "            embedding_dim=3)\n",
    "        self.comp_embedding_layer = nn.Embedding(\n",
    "            num_embeddings=len(comps),\n",
    "            embedding_dim=6)\n",
    "\n",
    "        self.include_numeric_feats = include_numeric_feats\n",
    "\n",
    "        predictor_input_dim = 28\n",
    "        if self.include_numeric_feats:\n",
    "            predictor_input_dim += input_dim\n",
    "\n",
    "        self.predictor_block = PredictorBlock(\n",
    "            predictor_input_dim,\n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            mlp_hidden_dim,\n",
    "            recurrent=recurrent,\n",
    "            dropout=dropout)\n",
    "\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def package_input_chunk(self, chunk):\n",
    "        code_tensor = self.code_embedding_layer(torch.tensor(chunk['Codice'].values, device=self.device).to(torch.long))\n",
    "        name_tensor = self.name_embedding_layer(torch.tensor(chunk['Nome'].values, device=self.device).to(torch.long))\n",
    "        desc_tensor = self.desc_embedding_layer(torch.tensor(chunk['Descrizione'].values, device=self.device).to(torch.long))\n",
    "        sist_tensor = self.sist_embedding_layer(torch.tensor(chunk['Sistema'].values, device=self.device).to(torch.long))\n",
    "        comp_tensor = self.comp_embedding_layer(torch.tensor(chunk['Componente'].values, device=self.device).to(torch.long))\n",
    "\n",
    "        cats_tensor = torch.hstack([\n",
    "            code_tensor,\n",
    "            name_tensor,\n",
    "            desc_tensor,\n",
    "            sist_tensor,\n",
    "            comp_tensor\n",
    "            ])\n",
    "\n",
    "        if self.include_numeric_feats:\n",
    "            nums_tensor = torch.tensor(\n",
    "                chunk.drop(columns=['Codice', 'Nome', 'Descrizione', 'Sistema', 'Componente']).values,\n",
    "                device=self.device)\n",
    "\n",
    "            input_tensor = torch.hstack([cats_tensor, nums_tensor])\n",
    "        else:\n",
    "            input_tensor = cats_tensor\n",
    "\n",
    "        return input_tensor.to(torch.float32).to(self.device)\n",
    "\n",
    "    def forward(self, chunk):\n",
    "\n",
    "        x = self.package_input_chunk(chunk).unsqueeze(0)\n",
    "        # assert not torch.all(torch.isnan(x))\n",
    "        x = self.predictor_block(x)\n",
    "        # assert not torch.all(torch.isnan(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbdecd14-9ae7-4f1e-a049-6e356515b150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment(hypers):\n",
    "\n",
    "    chunks, scaler = get_chunks(hypers['name'])\n",
    "\n",
    "    torch.manual_seed(hypers['seed'])\n",
    "    predictor = AnomalyPredictor(\n",
    "        input_dim=157,\n",
    "        dropout=hypers['dropout'],\n",
    "        recurrent=hypers['recurrency'],\n",
    "        include_numeric_feats=hypers['include_numeric_feats'])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    predictor_optimizer = torch.optim.Adam(\n",
    "        params=predictor.parameters(),\n",
    "        lr=hypers['learning_rate'])\n",
    "\n",
    "    run_tags = []\n",
    "\n",
    "    if hypers['include_numeric_feats']:\n",
    "        run_tags.append('NUMS')\n",
    "    if hypers['recurrency']:\n",
    "        run_tags.append('RECURRENT')\n",
    "    if hypers['watch']:\n",
    "        run_tags.append('WATCH')\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"RailRED\",\n",
    "        name=hypers['name']+'num'+str(hypers['include_numeric_feats']),\n",
    "        config=hypers,\n",
    "        mode=('online' if hypers['logging'] else 'disabled'),\n",
    "        tags=run_tags\n",
    "    )\n",
    "\n",
    "    if hypers['watch']:\n",
    "        wandb.watch(\n",
    "            models=predictor,\n",
    "            criterion=F.mse_loss,\n",
    "            log='all',\n",
    "            log_freq=100,\n",
    "            log_graph=True)\n",
    "\n",
    "    errors = []\n",
    "    for epoch in range(hypers['epochs']):\n",
    "        for chunk in tqdm(chunks):\n",
    "\n",
    "            inner_errors = []\n",
    "            for label in chunk['rel_time'].unique():\n",
    "                feat = chunk.loc[chunk['rel_time']==label].drop(\n",
    "                    columns='rel_time').copy()\n",
    "                label = scaler.transform(np.array(label).reshape(-1, 1))\n",
    "                if (label <= 1):\n",
    "                    pred = predictor(feat)\n",
    "                    label = torch.tensor(label, device='cuda:0').to(torch.float32)\n",
    "                    err = criterion(input=pred, target=label)\n",
    "\n",
    "                    predictor_optimizer.zero_grad()\n",
    "                    err.backward()\n",
    "                    predictor_optimizer.step()\n",
    "\n",
    "                    wandb.log({'error': err})\n",
    "                    inner_errors.append(err.item())\n",
    "\n",
    "            mean_err = torch.tensor(inner_errors).mean().item()\n",
    "            wandb.log({'mean error': err})\n",
    "            errors.append(mean_err)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae23af4-d55d-45bf-99b4-d87ad43a844b",
   "metadata": {},
   "source": [
    "## First model: \n",
    "__________\n",
    "just give me the set of signals... and use just attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf556a49-4f46-4082-ac4b-640ac7caa09f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for curr_anom in anomalies.keys():\n",
    "\n",
    "    logging = True\n",
    "    include_numeric_feats = False\n",
    "    recurrency = False\n",
    "\n",
    "    hypers = {\n",
    "        'name': curr_anom,\n",
    "        'logging': logging,\n",
    "        'learning_rate': 3e-4,\n",
    "        'dropout': 0.4,\n",
    "        'seed': 1,\n",
    "        'epochs': 5,\n",
    "        'include_numeric_feats': include_numeric_feats,\n",
    "        'recurrency': recurrency,\n",
    "        'watch': False\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        run_experiment(hypers)\n",
    "    except Exception as e:\n",
    "        print('aio: ', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4ece28-1038-49d4-a06a-74c284b89acb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Second model:\n",
    "___\n",
    "\n",
    "let's add recurrency to see if we do better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7169519-dfeb-4298-8086-1da61e0e8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for curr_anom in anomalies.keys():\n",
    "\n",
    "    logging = True\n",
    "    include_numeric_feats = False\n",
    "    recurrency = True\n",
    "\n",
    "    hypers = {\n",
    "        'name': curr_anom,\n",
    "        'logging': logging,\n",
    "        'learning_rate': 3e-4,\n",
    "        'dropout': 0.4,\n",
    "        'seed': 1,\n",
    "        'epochs': 5,\n",
    "        'include_numeric_feats': include_numeric_feats,\n",
    "        'recurrency': recurrency,\n",
    "        'watch': False\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        run_experiment(hypers)\n",
    "    except Exception as e:\n",
    "        print('aio: ', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e17189-12ed-4272-877e-1ce3500f9444",
   "metadata": {},
   "source": [
    "## Third model:\n",
    "___\n",
    "\n",
    "let's add other features to see if we do better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa1cf56-ef6f-4fd6-96b2-ffb9cb3645e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "for curr_anom in anomalies.keys():\n",
    "\n",
    "    logging = True\n",
    "    include_numeric_feats = True\n",
    "    recurrency = False\n",
    "\n",
    "    hypers = {\n",
    "        'name': curr_anom,\n",
    "        'logging': logging,\n",
    "        'learning_rate': 3e-4,\n",
    "        'dropout': 0.2,\n",
    "        'seed': 1,\n",
    "        'epochs': 5,\n",
    "        'include_numeric_feats': include_numeric_feats,\n",
    "        'recurrency': recurrency,\n",
    "        'watch': True\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        run_experiment(hypers)\n",
    "    except Exception as e:\n",
    "        print('aio: ', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd589588-6b31-4353-8878-50e4c15a6970",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fourth model:\n",
    "___________\n",
    "Adding recurrency...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c18de54-3318-4fe5-a793-e41220110355",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c818106da3ce4714895ff576710e45ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28079 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 106 data samples for anomaly: Alta tensione non presente in M1-T4\n",
      "length of samples: \n",
      "Tranining a scaler for interarrival times...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c95021985542f08777209fe8e762fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjfcevallos\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/NodeRed RW model/wandb/run-20240705_154222-gzvzc05m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jfcevallos/RailRED/runs/gzvzc05m' target=\"_blank\">Alta tensione non presente in M1-T4numTrue</a></strong> to <a href='https://wandb.ai/jfcevallos/RailRED' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jfcevallos/RailRED' target=\"_blank\">https://wandb.ai/jfcevallos/RailRED</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jfcevallos/RailRED/runs/gzvzc05m' target=\"_blank\">https://wandb.ai/jfcevallos/RailRED/runs/gzvzc05m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274595038d73460bb2dd52d889caca7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c712cb92eb4c5e9fe7d09622f9f96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d9ca6054404084802a552ae55af1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c700d5e605433a9eaad2096748e1e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ccb427d0e146b0813334e67bc0d622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.029 MB of 0.029 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>error</td><td>▁▁▃▂▁▂▁▆▁▂▃▃▁▂▁▂▁▁▂▄▁▁▂█▂▁▃▁▁▄▂▁▁▁▁▂▁▁▁▂</td></tr><tr><td>mean error</td><td>▁█▅▇▁▂▄▄▂▅▄▆▄▂▃▃▂▄▄▄▃▄▂▃▃▄▅▅▂▅▂▄▄▅▄▅▃▂▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>error</td><td>0.20892</td></tr><tr><td>mean error</td><td>0.20892</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Alta tensione non presente in M1-T4numTrue</strong> at: <a href='https://wandb.ai/jfcevallos/RailRED/runs/gzvzc05m' target=\"_blank\">https://wandb.ai/jfcevallos/RailRED/runs/gzvzc05m</a><br/> View project at: <a href='https://wandb.ai/jfcevallos/RailRED' target=\"_blank\">https://wandb.ai/jfcevallos/RailRED</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240705_154222-gzvzc05m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbde3b4e5e3a4bbca12efc8bec80adbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28079 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 108 data samples for anomaly: Alta tensione non presente in M8-T5\n",
      "length of samples: \n",
      "Tranining a scaler for interarrival times...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c596093d8e4771ac0cb864de1c64af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/NodeRed RW model/wandb/run-20240705_154614-3mrbmdla</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jfcevallos/RailRED/runs/3mrbmdla' target=\"_blank\">Alta tensione non presente in M8-T5numTrue</a></strong> to <a href='https://wandb.ai/jfcevallos/RailRED' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jfcevallos/RailRED' target=\"_blank\">https://wandb.ai/jfcevallos/RailRED</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jfcevallos/RailRED/runs/3mrbmdla' target=\"_blank\">https://wandb.ai/jfcevallos/RailRED/runs/3mrbmdla</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956c755ea67f4ab389298145b6846921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385be22dd934455099de1e8776cf6d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12d0a10b5524092adc0463500942c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca8bfa2d6a6476a81c56af481dff2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fb820e1a594fd8aef3cc56fd403ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.029 MB of 0.127 MB uploaded\\r'), FloatProgress(value=0.23109234225772274, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>error</td><td>▁▁▂▂▁▁▂▃▃▁▂▁▁▂▂▂▂▃▃▁▂▂▁▁▁▁▁▂▁▁▂▂▂▂▁▄▁█▂▂</td></tr><tr><td>mean error</td><td>▁▂█▃▃▅▆▃▂▄▅▆▅▄▄▄▃▃▅▅▅▅▃▃▄▄▅▅▅▄▄▄▄▄▄▅▅▅▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>error</td><td>0.26308</td></tr><tr><td>mean error</td><td>0.26308</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Alta tensione non presente in M8-T5numTrue</strong> at: <a href='https://wandb.ai/jfcevallos/RailRED/runs/3mrbmdla' target=\"_blank\">https://wandb.ai/jfcevallos/RailRED/runs/3mrbmdla</a><br/> View project at: <a href='https://wandb.ai/jfcevallos/RailRED' target=\"_blank\">https://wandb.ai/jfcevallos/RailRED</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240705_154614-3mrbmdla/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a770200eab84ccdbc422b42ff41b7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28079 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 58 data samples for anomaly: Assistenza EP fuori servizio \n",
      "length of samples: \n",
      "Tranining a scaler for interarrival times...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe797d55c304e8a8abd6d43515f99c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46fad1a2277c41959f18d96991e7003b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112729190952249, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/NodeRed RW model/wandb/run-20240705_154953-xdl7uyhn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jfcevallos/RailRED/runs/xdl7uyhn' target=\"_blank\">Assistenza EP fuori servizio numTrue</a></strong> to <a href='https://wandb.ai/jfcevallos/RailRED' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jfcevallos/RailRED' target=\"_blank\">https://wandb.ai/jfcevallos/RailRED</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jfcevallos/RailRED/runs/xdl7uyhn' target=\"_blank\">https://wandb.ai/jfcevallos/RailRED/runs/xdl7uyhn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508dafc17f2c414e8f520d8645c254d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9420a0933a6e42eabb2b8c031b475011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8139f6ba7e704ba6afc95a98c4643361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbab973f0c6f46b1af4f33b05afedc67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ecc7f7e759479e84eff7662e96959f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bda06bfd5b643a5a9f40574a8941f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.029 MB of 0.092 MB uploaded\\r'), FloatProgress(value=0.319246731687072, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>error</td><td>▂▂█▁▄▁▂▁▁▁▁▁▂▃▃▂▂▃▃▂▂▄▅▁▇▂▁▁▁▅▁▃▂▂▁▂▁▂▁▂</td></tr><tr><td>mean error</td><td>▁▆▃▄█▇▇█▄▁▆▃▅▄▆▅▄▅▃▂▅▃▅▅▅▅▅▄▄▃▅▅▅▅▅▅▄▄▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>error</td><td>0.33045</td></tr><tr><td>mean error</td><td>0.33045</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Assistenza EP fuori servizio numTrue</strong> at: <a href='https://wandb.ai/jfcevallos/RailRED/runs/xdl7uyhn' target=\"_blank\">https://wandb.ai/jfcevallos/RailRED/runs/xdl7uyhn</a><br/> View project at: <a href='https://wandb.ai/jfcevallos/RailRED' target=\"_blank\">https://wandb.ai/jfcevallos/RailRED</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240705_154953-xdl7uyhn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for curr_anom in list(anomalies.keys())[: 3]:\n",
    "\n",
    "    logging = True\n",
    "    include_numeric_feats = True\n",
    "    recurrency = True\n",
    "\n",
    "    hypers = {\n",
    "        'name': curr_anom,\n",
    "        'logging': logging,\n",
    "        'learning_rate': 3e-4,\n",
    "        'dropout': 0.4,\n",
    "        'seed': 1,\n",
    "        'epochs': 5,\n",
    "        'include_numeric_feats': include_numeric_feats,\n",
    "        'recurrency': recurrency,\n",
    "        'watch': True\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        run_experiment(hypers)\n",
    "    except Exception as e:\n",
    "        print('aio: ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb52a8b6-8ae2-4acf-9b4f-5e22384b5d07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /opt/conda/lib/python3.11/site-packages/sacremoses-0.0.43-py3.8.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting captum\n",
      "  Obtaining dependency information for captum from https://files.pythonhosted.org/packages/e1/76/b21bfd2c35cab2e9a4b68b1977f7488c246c8cffa31e3361ee7610e8b5af/captum-0.7.0-py3-none-any.whl.metadata\n",
      "  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (from captum) (3.8.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from captum) (1.24.4)\n",
      "Requirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.11/site-packages (from captum) (2.3.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from captum) (4.66.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.6->captum) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6->captum) (12.5.40)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->captum) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib->captum) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->captum) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->captum) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->captum) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->captum) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->captum) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.6->captum) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.6->captum) (1.3.0)\n",
      "Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: captum\n",
      "Successfully installed captum-0.7.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751db0d-53fe-4640-bfce-40bd94367b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions, approximation_error = ig.attribute((input1, input2),\n",
    "                                                 baselines=(baseline1, baseline2),\n",
    "                                                 method='gausslegendre',\n",
    "                                                 return_convergence_delta=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cfcdcc-1778-4e8a-af15-5a1c3a863cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers (Python 3.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
